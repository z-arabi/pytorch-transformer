{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmj8kG3zxMpHBdgbAGCsJg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/z-arabi/pytorch-transformer/blob/main/VisionTransformers/Swin_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the source code is: https://github.com/berniwal/swin-transformer-pytorch"
      ],
      "metadata": {
        "id": "4juJynQonQgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.8.1\n",
        "!pip install einops==0.3.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xc3JtWdynJxx",
        "outputId": "4031be7a-bfca-4717-f5e4-e2738da51c96"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.8.1 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.8.1\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting einops==0.3.0\n",
            "  Downloading einops-0.3.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: einops\n",
            "  Attempting uninstall: einops\n",
            "    Found existing installation: einops 0.8.0\n",
            "    Uninstalling einops-0.8.0:\n",
            "      Successfully uninstalled einops-0.8.0\n",
            "Successfully installed einops-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "co4E2By9nEc4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, einsum\n",
        "import numpy as np\n",
        "from einops import rearrange, repeat\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class CyclicShift(nn.Module):\n",
        "    def __init__(self, displacement):\n",
        "        super().__init__()\n",
        "        self.displacement = displacement\n",
        "\n",
        "    def forward(self, x):\n",
        "        # minus values shows the shifting to the right and down\n",
        "        # positive values > left and up\n",
        "        # we have the b,h,w,c > so the dim would be 1 and 2\n",
        "        return torch.roll(x, shifts=(self.displacement, self.displacement), dims=(1, 2))\n",
        "\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        # for the v2 of sein transformers the layer norm occurs after the attention > self.norm(self.fn(x,**kwargs)\n",
        "        return self.fn(self.norm(x), **kwargs) # that's prenorm for the v1 of swin transformers\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def create_mask(window_size, displacement, upper_lower, left_right):\n",
        "    mask = torch.zeros(window_size ** 2, window_size ** 2) # 49*49\n",
        "\n",
        "    if upper_lower:\n",
        "        mask[-displacement * window_size:, :-displacement * window_size] = float('-inf')\n",
        "        mask[:-displacement * window_size, -displacement * window_size:] = float('-inf')\n",
        "\n",
        "    if left_right:\n",
        "        mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size)\n",
        "        mask[:, -displacement:, :, :-displacement] = float('-inf')\n",
        "        mask[:, :-displacement, :, -displacement:] = float('-inf')\n",
        "        mask = rearrange(mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)')\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "def get_relative_distances(window_size):\n",
        "    indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)]))\n",
        "    # print('indices: ', indices.shape) # [49,2]\n",
        "    distances = indices[None, :, :] - indices[:, None, :]\n",
        "    print(\"distance\", distances.shape) # (49,49,2)\n",
        "    return distances\n",
        "\n",
        "\n",
        "class WindowAttention(nn.Module):\n",
        "    def __init__(self, dim, heads, head_dim, shifted, window_size, relative_pos_embedding):\n",
        "        # dim=hid_dimension = (96, 192, 384, 768)\n",
        "        # heads=number_heads = (3,6,12,24)\n",
        "        # head_dim=32\n",
        "        # window_size=7\n",
        "        super().__init__()\n",
        "        inner_dim = head_dim * heads # inner_dim == C == dim == hid_dimension == (32*3=96, 32*6=192, 32*12=384, 32*24=768)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = head_dim ** -0.5 # scaling in the softmax\n",
        "        self.window_size = window_size\n",
        "        self.relative_pos_embedding = relative_pos_embedding\n",
        "        self.shifted = shifted # shifting to the right and down with the value of half of the window size and pad them\n",
        "        # we have two kind of padding > naive(adding zero) and cyclic padding\n",
        "\n",
        "        if self.shifted:\n",
        "            displacement = window_size // 2 # 7//2=3\n",
        "            self.cyclic_shift = CyclicShift(-displacement)\n",
        "            self.cyclic_back_shift = CyclicShift(displacement)\n",
        "            # Persistence: The mask is stored as part of the model's state and will be included when saving/loading the model.\n",
        "            # not learnable\n",
        "            self.upper_lower_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
        "                                                             upper_lower=True, left_right=False), requires_grad=False)\n",
        "            self.left_right_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
        "                                                            upper_lower=False, left_right=True), requires_grad=False)\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "\n",
        "        # for each stage we have the positional embedding because previously we had the image . patchify it > then give the embedding to the model\n",
        "        if self.relative_pos_embedding:\n",
        "            # the full matrix distances: 49,49,2\n",
        "            # to have the indices in the correct range > the most neg > -6 (win-1) > add 6 to start from 0\n",
        "            self.relative_indices = get_relative_distances(window_size) + window_size - 1\n",
        "            # number of params are 13*13 instead of 49*49\n",
        "            self.pos_embedding = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1))\n",
        "        else:\n",
        "            # we don't need the cls token anymore and also embedding is different is has 49,49\n",
        "            self.pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2))\n",
        "\n",
        "        # inner_dim == dim >> Wo\n",
        "        self.to_out = nn.Linear(inner_dim, dim)\n",
        "\n",
        "        ## for the cosine similiarity\n",
        "        self.tau = nn.Parameter(torch.tensor(0.01), requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.shifted:\n",
        "            x = self.cyclic_shift(x) #(1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
        "\n",
        "        b, n_h, n_w, _, h = *x.shape, self.heads\n",
        "\n",
        "        # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96*3, 192*3, 384*3, 768*3))\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1) # chunk the last dim in order to have the same dim for all q,k,v\n",
        "\n",
        "        nw_h = n_h // self.window_size # (56//7=8, 28//7=4, 14//7=2, 7//7=1)\n",
        "        nw_w = n_w // self.window_size\n",
        "\n",
        "        # just to separate the head and dim for each head\n",
        "        #(b=1, h=(3,6,12,24), (nw_h*nw_w)=(64,16,4,1), (w_h*w_w)=49, d=32) where d=head_dim; h=#heads;\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, 'b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d',\n",
        "                                h=h, w_h=self.window_size, w_w=self.window_size), qkv)\n",
        "\n",
        "        # w stands for how many windows do we have in total\n",
        "        # d is the hidden dim\n",
        "        # we want to find the connection between all windows > ij\n",
        "        dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) * self.scale\n",
        "        # print('dots.size: ', dots.size()) # dots.size:  torch.Size([1, 3, 64, 49, 49])\n",
        "\n",
        "        # ## if the cosine similiarity:\n",
        "        # q = F.normalized(q, p=2, dim=-1)\n",
        "        # k = F.normalized(k, p=2, dim=-1)\n",
        "        # dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) / self.tau\n",
        "\n",
        "        # we add the positional embeddings after dot\n",
        "        if self.relative_pos_embedding:\n",
        "            # first you find the indexes and then you put each parameter based on its indices\n",
        "            # you have 13*13 params > [0,0] , [0,1], [0,2] , ...\n",
        "            dots += self.pos_embedding[self.relative_indices[:, :, 0], self.relative_indices[:, :, 1]]\n",
        "        else:\n",
        "            dots += self.pos_embedding\n",
        "\n",
        "        if self.shifted:\n",
        "            # last row windows > would be the last 8 windows in stage 1\n",
        "            dots[:, :, -nw_w:] += self.upper_lower_mask\n",
        "            # last column windows\n",
        "            dots[:, :, nw_w - 1::nw_w] += self.left_right_mask\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = einsum('b h w i j, b h w j d -> b h w i d', attn, v)\n",
        "        out = rearrange(out, 'b h (nw_h nw_w) (w_h w_w) d -> b (nw_h w_h) (nw_w w_w) (h d)',\n",
        "                        h=h, w_h=self.window_size, w_w=self.window_size, nw_h=nw_h, nw_w=nw_w)\n",
        "        out = self.to_out(out)\n",
        "\n",
        "        if self.shifted:\n",
        "            out = self.cyclic_back_shift(out)\n",
        "\n",
        "        #(1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
        "        return out\n",
        "\n",
        "\n",
        "class SwinBlock(nn.Module):\n",
        "    def __init__(self, dim, heads, head_dim, mlp_dim, shifted, window_size, relative_pos_embedding):\n",
        "        super().__init__()\n",
        "        self.attention_block = Residual(PreNorm(dim, WindowAttention(dim=dim,\n",
        "                                                                     heads=heads,\n",
        "                                                                     head_dim=head_dim,\n",
        "                                                                     shifted=shifted,\n",
        "                                                                     window_size=window_size,\n",
        "                                                                     relative_pos_embedding=relative_pos_embedding)))\n",
        "        self.mlp_block = Residual(PreNorm(dim, FeedForward(dim=dim, hidden_dim=mlp_dim)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.attention_block(x) #(1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
        "        x = self.mlp_block(x) #(1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchMerging(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downscaling_factor):\n",
        "        super().__init__()\n",
        "        self.downscaling_factor = downscaling_factor\n",
        "        self.patch_merge = nn.Unfold(kernel_size=downscaling_factor, stride=downscaling_factor, padding=0)\n",
        "        self.linear = nn.Linear(in_channels * downscaling_factor ** 2, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        new_h, new_w = h // self.downscaling_factor, w // self.downscaling_factor\n",
        "        x = self.patch_merge(x).view(b, -1, new_h, new_w).permute(0, 2, 3, 1)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "class PatchMerging_Conv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downscaling_factor):\n",
        "        super().__init__()\n",
        "\n",
        "        # not overlapping kernels\n",
        "        self.patch_merge = nn.Conv2d(in_channels,\n",
        "                                     out_channels,\n",
        "                                     kernel_size=downscaling_factor,\n",
        "                                     stride=downscaling_factor,\n",
        "                                     padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print('x.size in patch_merging: ', x.size()) # (1, (3,96,192,384), (224,56,28,14), (224,56,28,14))\n",
        "        #self.patch_merge(x) # (1, (96, 192, 384, 768), (56, 28, 14, 7), (56, 28, 14, 7))\n",
        "\n",
        "        # conv2d layer format input: (N, C, H, W) >> why permuting for the output???\n",
        "        x = self.patch_merge(x).permute(0, 2, 3, 1) # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768)) > b,h,w,c\n",
        "        print('x.size after patch_merging: ', x.size())\n",
        "        return x\n",
        "\n",
        "class StageModule(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_dimension, layers, downscaling_factor, num_heads, head_dim, window_size,\n",
        "                 relative_pos_embedding):\n",
        "        super().__init__()\n",
        "        # layers > how many swin block in one stage > we have to assign the pairs of the regular and shifting ones > it must be even\n",
        "        assert layers % 2 == 0, 'Stage layers need to be divisible by 2 for regular and shifted block.'\n",
        "\n",
        "        self.patch_partition = PatchMerging_Conv(in_channels=in_channels, out_channels=hidden_dimension,\n",
        "                                            downscaling_factor=downscaling_factor)\n",
        "\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(layers // 2):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
        "                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
        "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
        "                          shifted=True, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # the values for each stage > bs,3,224,224 > bs,96,56,56 > bs,192,28,28 > bs,14,14,382\n",
        "        # the format is #(1, (3, 96, 192, 384), (224, 56, 28, 14), (224, 56, 28, 14))\n",
        "        # change the size and create the hierarchical > The input of the transformer has the same shape of the output of that\n",
        "        # for stage one you can merge the patch_partition to the linear embedding > the target: change the dimensions\n",
        "        # What is the difference between patch partition and the linear embedding ????\n",
        "        x = self.patch_partition(x)\n",
        "        #(1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
        "        # for conv > b,c,h,w\n",
        "        # for the multihead attention > b,h,w,c\n",
        "        for regular_block, shifted_block in self.layers:\n",
        "            x = regular_block(x) #(1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
        "            x = shifted_block(x) #(1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
        "        return x.permute(0, 3, 1, 2) # Final output for the last stage (1, 768, 7, 7)\n",
        "\n",
        "\n",
        "class SwinTransformer(nn.Module):\n",
        "    # downscaling factor for the H and W > /4 > /8 > /16 > /32\n",
        "    # window_size > last feature should have a size of 7 > H/32=7\n",
        "    # channel sizes > hid_dim=96 > *2 > *4 > *8=768\n",
        "    def __init__(self, *, hidden_dim, layers, heads, channels=3, num_classes=1000, head_dim=32, window_size=7,\n",
        "                 downscaling_factors=(4, 2, 2, 2), relative_pos_embedding=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # four stages of swin blocks\n",
        "        # layers=(2, 2, 6, 2), heads=(3, 6, 12, 24)\n",
        "        # initial hid_dim = 96 > *2 > *4 > *8\n",
        "        self.stage1 = StageModule(in_channels=channels, hidden_dimension=hidden_dim, layers=layers[0],\n",
        "                                  downscaling_factor=downscaling_factors[0], num_heads=heads[0], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "\n",
        "        self.stage2 = StageModule(in_channels=hidden_dim, hidden_dimension=hidden_dim * 2, layers=layers[1],\n",
        "                                  downscaling_factor=downscaling_factors[1], num_heads=heads[1], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "\n",
        "        self.stage3 = StageModule(in_channels=hidden_dim * 2, hidden_dimension=hidden_dim * 4, layers=layers[2],\n",
        "                                  downscaling_factor=downscaling_factors[2], num_heads=heads[2], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "\n",
        "        self.stage4 = StageModule(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, layers=layers[3],\n",
        "                                  downscaling_factor=downscaling_factors[3], num_heads=heads[3], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "\n",
        "        # classification head\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim * 8),\n",
        "            nn.Linear(hidden_dim * 8, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        # what is the shape of the img? for each block we are giving the image size > BS H W 3 > bs,3,224,224\n",
        "        print(\"Input image shape:\", img.shape)\n",
        "        x = self.stage1(img)\n",
        "        x = self.stage2(x)\n",
        "        x = self.stage3(x)\n",
        "        x = self.stage4(x)\n",
        "        # why mean??? [it's ok to feed the 1,768,49 to the MLP]\n",
        "        print(\"X of the last stage\", x.shape) #1, 768, 7, 7\n",
        "        x = x.mean(dim=[2, 3])\n",
        "        print(\"X after mean\", x.shape) #1, 768\n",
        "        return self.mlp_head(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# layers > how many swin blocks in each stage\n",
        "# input_channel = 3\n",
        "# but the hidden_dim = 96 > C in the paper\n",
        "# heads is for the number of heads\n",
        "def swin_t(hidden_dim=96, layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
        "\n",
        "\n",
        "def swin_s(hidden_dim=96, layers=(2, 2, 18, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
        "\n",
        "\n",
        "def swin_b(hidden_dim=128, layers=(2, 2, 18, 2), heads=(4, 8, 16, 32), **kwargs):\n",
        "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
        "\n",
        "\n",
        "def swin_l(hidden_dim=192, layers=(2, 2, 18, 2), heads=(6, 12, 24, 48), **kwargs):\n",
        "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
        "\n",
        "\n",
        "model = swin_t(num_classes=3)\n",
        "sample = torch.randn(1, 3, 224, 224)\n",
        "output = model(sample)\n",
        "print(output.shape)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBIhpAlUnpxo",
        "outputId": "aae81b0c-c168-4d12-c8be-b5268521ed4f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input image shape: torch.Size([1, 3, 224, 224])\n",
            "x.size in patch_merging:  torch.Size([1, 3, 224, 224])\n",
            "x.size after patch_merging:  torch.Size([1, 56, 56, 96])\n",
            "dots.size:  torch.Size([1, 3, 64, 49, 49])\n",
            "dots.size:  torch.Size([1, 3, 64, 49, 49])\n",
            "x.size in patch_merging:  torch.Size([1, 96, 56, 56])\n",
            "x.size after patch_merging:  torch.Size([1, 28, 28, 192])\n",
            "dots.size:  torch.Size([1, 6, 16, 49, 49])\n",
            "dots.size:  torch.Size([1, 6, 16, 49, 49])\n",
            "x.size in patch_merging:  torch.Size([1, 192, 28, 28])\n",
            "x.size after patch_merging:  torch.Size([1, 14, 14, 384])\n",
            "dots.size:  torch.Size([1, 12, 4, 49, 49])\n",
            "dots.size:  torch.Size([1, 12, 4, 49, 49])\n",
            "dots.size:  torch.Size([1, 12, 4, 49, 49])\n",
            "dots.size:  torch.Size([1, 12, 4, 49, 49])\n",
            "dots.size:  torch.Size([1, 12, 4, 49, 49])\n",
            "dots.size:  torch.Size([1, 12, 4, 49, 49])\n",
            "x.size in patch_merging:  torch.Size([1, 384, 14, 14])\n",
            "x.size after patch_merging:  torch.Size([1, 7, 7, 768])\n",
            "dots.size:  torch.Size([1, 24, 1, 49, 49])\n",
            "dots.size:  torch.Size([1, 24, 1, 49, 49])\n",
            "X of the last stage torch.Size([1, 768, 7, 7])\n",
            "X after mean torch.Size([1, 768])\n",
            "torch.Size([1, 3])\n",
            "tensor([[ 0.2857, -0.5377, -0.8097]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torchsummary import summary\n",
        "# Total params: 28,247,560\n",
        "summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4wDBduIDLXm4",
        "outputId": "7c607095-1b04-413a-8843-ac0d439e221a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input image shape: torch.Size([2, 3, 224, 224])\n",
            "x.size in patch_merging:  torch.Size([2, 3, 224, 224])\n",
            "x.size after patch_merging:  torch.Size([2, 56, 56, 96])\n",
            "x.size in patch_merging:  torch.Size([2, 96, 56, 56])\n",
            "x.size after patch_merging:  torch.Size([2, 28, 28, 192])\n",
            "x.size in patch_merging:  torch.Size([2, 192, 28, 28])\n",
            "x.size after patch_merging:  torch.Size([2, 14, 14, 384])\n",
            "x.size in patch_merging:  torch.Size([2, 384, 14, 14])\n",
            "x.size after patch_merging:  torch.Size([2, 7, 7, 768])\n",
            "X of the last stage torch.Size([2, 768, 7, 7])\n",
            "X after mean torch.Size([2, 768])\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 96, 56, 56]           4,704\n",
            " PatchMerging_Conv-2           [-1, 56, 56, 96]               0\n",
            "         LayerNorm-3           [-1, 56, 56, 96]             192\n",
            "            Linear-4          [-1, 56, 56, 288]          27,648\n",
            "            Linear-5           [-1, 56, 56, 96]           9,312\n",
            "   WindowAttention-6           [-1, 56, 56, 96]               0\n",
            "           PreNorm-7           [-1, 56, 56, 96]               0\n",
            "          Residual-8           [-1, 56, 56, 96]               0\n",
            "         LayerNorm-9           [-1, 56, 56, 96]             192\n",
            "           Linear-10          [-1, 56, 56, 384]          37,248\n",
            "             GELU-11          [-1, 56, 56, 384]               0\n",
            "           Linear-12           [-1, 56, 56, 96]          36,960\n",
            "      FeedForward-13           [-1, 56, 56, 96]               0\n",
            "          PreNorm-14           [-1, 56, 56, 96]               0\n",
            "         Residual-15           [-1, 56, 56, 96]               0\n",
            "        SwinBlock-16           [-1, 56, 56, 96]               0\n",
            "        LayerNorm-17           [-1, 56, 56, 96]             192\n",
            "      CyclicShift-18           [-1, 56, 56, 96]               0\n",
            "           Linear-19          [-1, 56, 56, 288]          27,648\n",
            "           Linear-20           [-1, 56, 56, 96]           9,312\n",
            "      CyclicShift-21           [-1, 56, 56, 96]               0\n",
            "  WindowAttention-22           [-1, 56, 56, 96]               0\n",
            "          PreNorm-23           [-1, 56, 56, 96]               0\n",
            "         Residual-24           [-1, 56, 56, 96]               0\n",
            "        LayerNorm-25           [-1, 56, 56, 96]             192\n",
            "           Linear-26          [-1, 56, 56, 384]          37,248\n",
            "             GELU-27          [-1, 56, 56, 384]               0\n",
            "           Linear-28           [-1, 56, 56, 96]          36,960\n",
            "      FeedForward-29           [-1, 56, 56, 96]               0\n",
            "          PreNorm-30           [-1, 56, 56, 96]               0\n",
            "         Residual-31           [-1, 56, 56, 96]               0\n",
            "        SwinBlock-32           [-1, 56, 56, 96]               0\n",
            "      StageModule-33           [-1, 96, 56, 56]               0\n",
            "           Conv2d-34          [-1, 192, 28, 28]          73,920\n",
            "PatchMerging_Conv-35          [-1, 28, 28, 192]               0\n",
            "        LayerNorm-36          [-1, 28, 28, 192]             384\n",
            "           Linear-37          [-1, 28, 28, 576]         110,592\n",
            "           Linear-38          [-1, 28, 28, 192]          37,056\n",
            "  WindowAttention-39          [-1, 28, 28, 192]               0\n",
            "          PreNorm-40          [-1, 28, 28, 192]               0\n",
            "         Residual-41          [-1, 28, 28, 192]               0\n",
            "        LayerNorm-42          [-1, 28, 28, 192]             384\n",
            "           Linear-43          [-1, 28, 28, 768]         148,224\n",
            "             GELU-44          [-1, 28, 28, 768]               0\n",
            "           Linear-45          [-1, 28, 28, 192]         147,648\n",
            "      FeedForward-46          [-1, 28, 28, 192]               0\n",
            "          PreNorm-47          [-1, 28, 28, 192]               0\n",
            "         Residual-48          [-1, 28, 28, 192]               0\n",
            "        SwinBlock-49          [-1, 28, 28, 192]               0\n",
            "        LayerNorm-50          [-1, 28, 28, 192]             384\n",
            "      CyclicShift-51          [-1, 28, 28, 192]               0\n",
            "           Linear-52          [-1, 28, 28, 576]         110,592\n",
            "           Linear-53          [-1, 28, 28, 192]          37,056\n",
            "      CyclicShift-54          [-1, 28, 28, 192]               0\n",
            "  WindowAttention-55          [-1, 28, 28, 192]               0\n",
            "          PreNorm-56          [-1, 28, 28, 192]               0\n",
            "         Residual-57          [-1, 28, 28, 192]               0\n",
            "        LayerNorm-58          [-1, 28, 28, 192]             384\n",
            "           Linear-59          [-1, 28, 28, 768]         148,224\n",
            "             GELU-60          [-1, 28, 28, 768]               0\n",
            "           Linear-61          [-1, 28, 28, 192]         147,648\n",
            "      FeedForward-62          [-1, 28, 28, 192]               0\n",
            "          PreNorm-63          [-1, 28, 28, 192]               0\n",
            "         Residual-64          [-1, 28, 28, 192]               0\n",
            "        SwinBlock-65          [-1, 28, 28, 192]               0\n",
            "      StageModule-66          [-1, 192, 28, 28]               0\n",
            "           Conv2d-67          [-1, 384, 14, 14]         295,296\n",
            "PatchMerging_Conv-68          [-1, 14, 14, 384]               0\n",
            "        LayerNorm-69          [-1, 14, 14, 384]             768\n",
            "           Linear-70         [-1, 14, 14, 1152]         442,368\n",
            "           Linear-71          [-1, 14, 14, 384]         147,840\n",
            "  WindowAttention-72          [-1, 14, 14, 384]               0\n",
            "          PreNorm-73          [-1, 14, 14, 384]               0\n",
            "         Residual-74          [-1, 14, 14, 384]               0\n",
            "        LayerNorm-75          [-1, 14, 14, 384]             768\n",
            "           Linear-76         [-1, 14, 14, 1536]         591,360\n",
            "             GELU-77         [-1, 14, 14, 1536]               0\n",
            "           Linear-78          [-1, 14, 14, 384]         590,208\n",
            "      FeedForward-79          [-1, 14, 14, 384]               0\n",
            "          PreNorm-80          [-1, 14, 14, 384]               0\n",
            "         Residual-81          [-1, 14, 14, 384]               0\n",
            "        SwinBlock-82          [-1, 14, 14, 384]               0\n",
            "        LayerNorm-83          [-1, 14, 14, 384]             768\n",
            "      CyclicShift-84          [-1, 14, 14, 384]               0\n",
            "           Linear-85         [-1, 14, 14, 1152]         442,368\n",
            "           Linear-86          [-1, 14, 14, 384]         147,840\n",
            "      CyclicShift-87          [-1, 14, 14, 384]               0\n",
            "  WindowAttention-88          [-1, 14, 14, 384]               0\n",
            "          PreNorm-89          [-1, 14, 14, 384]               0\n",
            "         Residual-90          [-1, 14, 14, 384]               0\n",
            "        LayerNorm-91          [-1, 14, 14, 384]             768\n",
            "           Linear-92         [-1, 14, 14, 1536]         591,360\n",
            "             GELU-93         [-1, 14, 14, 1536]               0\n",
            "           Linear-94          [-1, 14, 14, 384]         590,208\n",
            "      FeedForward-95          [-1, 14, 14, 384]               0\n",
            "          PreNorm-96          [-1, 14, 14, 384]               0\n",
            "         Residual-97          [-1, 14, 14, 384]               0\n",
            "        SwinBlock-98          [-1, 14, 14, 384]               0\n",
            "        LayerNorm-99          [-1, 14, 14, 384]             768\n",
            "          Linear-100         [-1, 14, 14, 1152]         442,368\n",
            "          Linear-101          [-1, 14, 14, 384]         147,840\n",
            " WindowAttention-102          [-1, 14, 14, 384]               0\n",
            "         PreNorm-103          [-1, 14, 14, 384]               0\n",
            "        Residual-104          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-105          [-1, 14, 14, 384]             768\n",
            "          Linear-106         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-107         [-1, 14, 14, 1536]               0\n",
            "          Linear-108          [-1, 14, 14, 384]         590,208\n",
            "     FeedForward-109          [-1, 14, 14, 384]               0\n",
            "         PreNorm-110          [-1, 14, 14, 384]               0\n",
            "        Residual-111          [-1, 14, 14, 384]               0\n",
            "       SwinBlock-112          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-113          [-1, 14, 14, 384]             768\n",
            "     CyclicShift-114          [-1, 14, 14, 384]               0\n",
            "          Linear-115         [-1, 14, 14, 1152]         442,368\n",
            "          Linear-116          [-1, 14, 14, 384]         147,840\n",
            "     CyclicShift-117          [-1, 14, 14, 384]               0\n",
            " WindowAttention-118          [-1, 14, 14, 384]               0\n",
            "         PreNorm-119          [-1, 14, 14, 384]               0\n",
            "        Residual-120          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-121          [-1, 14, 14, 384]             768\n",
            "          Linear-122         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-123         [-1, 14, 14, 1536]               0\n",
            "          Linear-124          [-1, 14, 14, 384]         590,208\n",
            "     FeedForward-125          [-1, 14, 14, 384]               0\n",
            "         PreNorm-126          [-1, 14, 14, 384]               0\n",
            "        Residual-127          [-1, 14, 14, 384]               0\n",
            "       SwinBlock-128          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-129          [-1, 14, 14, 384]             768\n",
            "          Linear-130         [-1, 14, 14, 1152]         442,368\n",
            "          Linear-131          [-1, 14, 14, 384]         147,840\n",
            " WindowAttention-132          [-1, 14, 14, 384]               0\n",
            "         PreNorm-133          [-1, 14, 14, 384]               0\n",
            "        Residual-134          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-135          [-1, 14, 14, 384]             768\n",
            "          Linear-136         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-137         [-1, 14, 14, 1536]               0\n",
            "          Linear-138          [-1, 14, 14, 384]         590,208\n",
            "     FeedForward-139          [-1, 14, 14, 384]               0\n",
            "         PreNorm-140          [-1, 14, 14, 384]               0\n",
            "        Residual-141          [-1, 14, 14, 384]               0\n",
            "       SwinBlock-142          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-143          [-1, 14, 14, 384]             768\n",
            "     CyclicShift-144          [-1, 14, 14, 384]               0\n",
            "          Linear-145         [-1, 14, 14, 1152]         442,368\n",
            "          Linear-146          [-1, 14, 14, 384]         147,840\n",
            "     CyclicShift-147          [-1, 14, 14, 384]               0\n",
            " WindowAttention-148          [-1, 14, 14, 384]               0\n",
            "         PreNorm-149          [-1, 14, 14, 384]               0\n",
            "        Residual-150          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-151          [-1, 14, 14, 384]             768\n",
            "          Linear-152         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-153         [-1, 14, 14, 1536]               0\n",
            "          Linear-154          [-1, 14, 14, 384]         590,208\n",
            "     FeedForward-155          [-1, 14, 14, 384]               0\n",
            "         PreNorm-156          [-1, 14, 14, 384]               0\n",
            "        Residual-157          [-1, 14, 14, 384]               0\n",
            "       SwinBlock-158          [-1, 14, 14, 384]               0\n",
            "     StageModule-159          [-1, 384, 14, 14]               0\n",
            "          Conv2d-160            [-1, 768, 7, 7]       1,180,416\n",
            "PatchMerging_Conv-161            [-1, 7, 7, 768]               0\n",
            "       LayerNorm-162            [-1, 7, 7, 768]           1,536\n",
            "          Linear-163           [-1, 7, 7, 2304]       1,769,472\n",
            "          Linear-164            [-1, 7, 7, 768]         590,592\n",
            " WindowAttention-165            [-1, 7, 7, 768]               0\n",
            "         PreNorm-166            [-1, 7, 7, 768]               0\n",
            "        Residual-167            [-1, 7, 7, 768]               0\n",
            "       LayerNorm-168            [-1, 7, 7, 768]           1,536\n",
            "          Linear-169           [-1, 7, 7, 3072]       2,362,368\n",
            "            GELU-170           [-1, 7, 7, 3072]               0\n",
            "          Linear-171            [-1, 7, 7, 768]       2,360,064\n",
            "     FeedForward-172            [-1, 7, 7, 768]               0\n",
            "         PreNorm-173            [-1, 7, 7, 768]               0\n",
            "        Residual-174            [-1, 7, 7, 768]               0\n",
            "       SwinBlock-175            [-1, 7, 7, 768]               0\n",
            "       LayerNorm-176            [-1, 7, 7, 768]           1,536\n",
            "     CyclicShift-177            [-1, 7, 7, 768]               0\n",
            "          Linear-178           [-1, 7, 7, 2304]       1,769,472\n",
            "          Linear-179            [-1, 7, 7, 768]         590,592\n",
            "     CyclicShift-180            [-1, 7, 7, 768]               0\n",
            " WindowAttention-181            [-1, 7, 7, 768]               0\n",
            "         PreNorm-182            [-1, 7, 7, 768]               0\n",
            "        Residual-183            [-1, 7, 7, 768]               0\n",
            "       LayerNorm-184            [-1, 7, 7, 768]           1,536\n",
            "          Linear-185           [-1, 7, 7, 3072]       2,362,368\n",
            "            GELU-186           [-1, 7, 7, 3072]               0\n",
            "          Linear-187            [-1, 7, 7, 768]       2,360,064\n",
            "     FeedForward-188            [-1, 7, 7, 768]               0\n",
            "         PreNorm-189            [-1, 7, 7, 768]               0\n",
            "        Residual-190            [-1, 7, 7, 768]               0\n",
            "       SwinBlock-191            [-1, 7, 7, 768]               0\n",
            "     StageModule-192            [-1, 768, 7, 7]               0\n",
            "       LayerNorm-193                  [-1, 768]           1,536\n",
            "          Linear-194                 [-1, 1000]         769,000\n",
            "================================================================\n",
            "Total params: 28,247,560\n",
            "Trainable params: 28,247,560\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 263.87\n",
            "Params size (MB): 107.76\n",
            "Estimated Total Size (MB): 372.20\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "B, H, W, C = 1, 2, 2, 3\n",
        "input = torch.randn(B, H, W, C) * 100\n",
        "print('input: ', input.shape)\n",
        "print(input)\n",
        "layer_norm = nn.LayerNorm(C)\n",
        "output = layer_norm(input)\n",
        "print('output: ', output.shape)\n",
        "print(output)\n",
        "\n",
        "# Layer norm has the gamma and beta that these can be the parameters for that\n",
        "x_mean = input.mean(dim=-1, keepdim=True)\n",
        "x_std = input.std(dim=-1, keepdim=True)\n",
        "# normalize the input based on these values\n",
        "x_ = (input - x_mean) / x_std\n",
        "print('x_: ', x_.shape)\n",
        "print(x_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUMYOWVCLZF_",
        "outputId": "45a0b49f-b8f7-4690-bfbc-6b047548e55f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:  torch.Size([1, 2, 2, 3])\n",
            "tensor([[[[ 154.0996,  -29.3429, -217.8789],\n",
            "          [  56.8431, -108.4522, -139.8595]],\n",
            "\n",
            "         [[  40.3347,   83.8026,  -71.9258],\n",
            "          [ -40.3344,  -59.6635,   18.2036]]]])\n",
            "output:  torch.Size([1, 2, 2, 3])\n",
            "tensor([[[[ 1.2191,  0.0112, -1.2303],\n",
            "          [ 1.3985, -0.5173, -0.8813]],\n",
            "\n",
            "         [[ 0.3495,  1.0120, -1.3615],\n",
            "          [-0.3948, -0.9787,  1.3735]]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "x_:  torch.Size([1, 2, 2, 3])\n",
            "tensor([[[[ 0.9954,  0.0091, -1.0045],\n",
            "          [ 1.1419, -0.4223, -0.7195]],\n",
            "\n",
            "         [[ 0.2854,  0.8263, -1.1117],\n",
            "          [-0.3223, -0.7991,  1.1214]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_mask = create_mask(window_size=7, displacement=3, upper_lower=True, left_right=False)\n",
        "print(new_mask)\n",
        "print(new_mask.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCLBty_VOONy",
        "outputId": "82c42239-42a6-480a-bb3f-9d3dc1ac4dad"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
            "        ...,\n",
            "        [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
            "        [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
            "        [-inf, -inf, -inf,  ..., 0., 0., 0.]])\n",
            "torch.Size([49, 49])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relative_indexes = get_relative_distances(3)\n",
        "print(relative_indexes[:,:,0])\n",
        "print(\"----\")\n",
        "print(relative_indexes[:,:,1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddwuy9m7brLR",
        "outputId": "5c360f0e-ea1c-4562-c744-d9cbaadd7b02"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distance torch.Size([9, 9, 2])\n",
            "tensor([[ 0,  0,  0,  1,  1,  1,  2,  2,  2],\n",
            "        [ 0,  0,  0,  1,  1,  1,  2,  2,  2],\n",
            "        [ 0,  0,  0,  1,  1,  1,  2,  2,  2],\n",
            "        [-1, -1, -1,  0,  0,  0,  1,  1,  1],\n",
            "        [-1, -1, -1,  0,  0,  0,  1,  1,  1],\n",
            "        [-1, -1, -1,  0,  0,  0,  1,  1,  1],\n",
            "        [-2, -2, -2, -1, -1, -1,  0,  0,  0],\n",
            "        [-2, -2, -2, -1, -1, -1,  0,  0,  0],\n",
            "        [-2, -2, -2, -1, -1, -1,  0,  0,  0]])\n",
            "----\n",
            "tensor([[ 0,  1,  2,  0,  1,  2,  0,  1,  2],\n",
            "        [-1,  0,  1, -1,  0,  1, -1,  0,  1],\n",
            "        [-2, -1,  0, -2, -1,  0, -2, -1,  0],\n",
            "        [ 0,  1,  2,  0,  1,  2,  0,  1,  2],\n",
            "        [-1,  0,  1, -1,  0,  1, -1,  0,  1],\n",
            "        [-2, -1,  0, -2, -1,  0, -2, -1,  0],\n",
            "        [ 0,  1,  2,  0,  1,  2,  0,  1,  2],\n",
            "        [-1,  0,  1, -1,  0,  1, -1,  0,  1],\n",
            "        [-2, -1,  0, -2, -1,  0, -2, -1,  0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pos_embedding       # (13, 13)\n",
        "p = torch.tensor([[1, 2],\n",
        "                  [3, 4]])\n",
        "print(p.size())\n",
        "\n",
        "# relative_indices is (49, 49, 2)\n",
        "r = torch.tensor([[[0,0],[0,0],[0,0]],\n",
        "                  [[1,1],[1,1],[1,1]],\n",
        "                  [[0,1],[0,1],[0,1]]])\n",
        "print(r.size())\n",
        "\n",
        "# self.pos_embedding[self.relative_indices[:, :, 0], self.relative_indices[:, :, 1]] #(49, 49)\n",
        "print(p[r[:,:,0], r[:,:,1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnMTeWXYRHDV",
        "outputId": "023c872d-f570-4fef-9950-3519d5e50be5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2])\n",
            "torch.Size([3, 3, 2])\n",
            "tensor([[1, 1, 1],\n",
            "        [4, 4, 4],\n",
            "        [2, 2, 2]])\n"
          ]
        }
      ]
    }
  ]
}